library(shiny)
library(bslib)

library(dplyr)
library(pROC)
library(ggplot2)
library(ggbiplot)
library(svglite)
library(tidyr)
library(bio3d)
library(mclust)
library(naivebayes)
library(psych)
library(randomForest)
library(caret)
library(purrr)
library(tidyverse)
library("klaR")

# this app performs an initial analysis on integrating multiple MAVEs for the same gene

# Define UI for slider demo app ----
ui <- page_sidebar(
  
  # App title ----
  title = "Integrate multiple MAVE datasets with sample methods | current version 1.6",
  
  # Sidebar panel for inputs ----
  sidebar = sidebar(
    
    # Input: Select csv files ----
    fileInput(
      "files",
      "Choose csv files | Multiple MAVE datasets, same gene",
      multiple = TRUE,
      accept = c(".csv")
    ),
    
    # Horizontal line ----
    hr(),
    numericInput("whatSeed", label = h6("What seed to set for random train/test split? default=1234"), value = 1234),
    numericInput("num", label = h6("How many clusters for k-means? default=2"), value = 2),
    numericInput("num2", label = h6("How many random sets for k-means? default=20"), value = 20),
    #radioButtons("radio", label = h6("Which algorithm for GFMM?"),
    #             choices = list("MclustDA (Default)" = 1, "EDDA" = 2), 
    #             selected = 1),
    numericInput("num3", label = h6("What percent of variants for training (0.1-0.9)? default=0.8"), value = 0.8),
    numericInput("num4", label = h6("How many trees for random forest? default=500"), value = 500),
    numericInput("num5", label = h6("What stepFactor for random forest? default=1.2"), value = 1.2),
    numericInput("num6", label = h6("What relative improvement to continue search? default=0.01"), value = 0.01),
        hr(),
    
    fluidPage(
      #tags$head(tags$script(src = "message-handler.js")),
      actionButton("do", "Click here to perform analysis")
    ),
    fluidPage(
      downloadButton("downloadData", "After analysis, click here to download model metrics")
    ),
  fluidPage(
    downloadButton("downloadData2", "After analysis, click here to download variant level dataframe")
  )),
    mainPanel( # add more plots!
      plotOutput('plot1'),
      plotOutput('plot2'),
      plotOutput('plot3'),
      plotOutput('plot4'),
      #textOutput("NB_intro"),
      #textOutput("NV_cv"),
      textOutput("RF_intro"),
      textOutput("RF_cv")
    )
  )

# Define server logic to read selected file ----
server <- function(input, output, session) {
  observeEvent(input$do, {
    
    req(input$files)
    
    ####### input app code here ########
    
    # we are going to specify in the instructions how users format their input data
    ## to match the style of our TP53 scores csv
    ### and any duplicates need to be collapsed too with dplyr summarise or other method
    
    # sample code for collapsing duplicates
    
    #tibble1 <- df1 %>%
    #  group_by(hgvs_pro) %>%
    #  dplyr::summarise(mean = mean(score), n = n())
    
    # read in multiple files and merge
    print(length(input$files[,1])) # troubleshooting
    print(input$files) # troubleshooting
    lst = list()
    for(i in 1:length(input$files[,1])){
      #print(read.csv(file=input$files[[i, 'datapath']],sep=",")) # troubleshooting
      lst[[i]] <- read.csv(file=input$files[[i, 'datapath']],sep=",")
      print(lst[[i]]) # troubleshooting
    }
    
    # merge multiple dataframes in a list
    #list(x, y, z) %>% reduce(left_join, by = "i")
    merged.data.frame <- lst %>% reduce(left_join, by = "hgvs_pro")
    #merged.data.frame <- Reduce(function(...) merge(..., all=T), lst)
    print(head(merged.data.frame)) # troubleshooting
    print(tail(merged.data.frame)) # troubleshooting
    
    # sample code for merging MAVE dataframes
    
    #df1 <- read.csv(file="MAVE1.csv", sep=",")
    #df2 <- read.csv(file="MAVE2.csv", sep=",")
    #merge_df <- merge(df1,df2,by="hgvs_pro")
    
    # need to drop NAs
    
    no_na_df <- merged.data.frame %>% drop_na()
    
    ################
    ################
    ################
    
    # read in data ## will likely need to modify this for Shiny app
    ### need users to upload their own custom csv
    #df1 <- read.csv(file="/Users/jeffreycalhoun/Downloads/merge_5xTP53maves.csv", sep=",")
    #df1 <- read.csv(file=input$file1$datapath,sep=",")
    df1 <- no_na_df
      
    # make a version of dataframe without the class column or variant name column
    drops <- c("binary_clinvar_class","hgvs_pro")
    df_subset <- df1[ , !(names(df1) %in% drops)]
    
    
    ######################
    ######################
    ###################### PCA / kmeans
    
    # scale the data 
    data_scaled = scale(df_subset)
    
    # w scaling #PCA
    pc <- prcomp(data_scaled,
                 center = TRUE,
                 scale. = TRUE)
    
    # add PC1 to newdataframe
    df_wPC1 <- df_subset
    df_wPC1$PC1 <- pc$x[,1]
    no_na_df$PC1 <- df_wPC1$PC1
    # Kmeans ## w data scaling
    
    set.seed(12)
    
    # maybe add user input to customize parameters for K means clustering?
    kCluster <- kmeans(data_scaled, input$num, nstart = input$num2)
    kCluster
    kCluster$cluster <- as.factor(kCluster$cluster)
    table(kCluster$cluster, df1$binary_clinvar_class)
    
    #     B   P
    # 1   0 113
    # 2  32  16
    no_na_df$kMeansCluster <- kCluster$cluster
    # kmeans clustering plot ## color points by kmeans assign cluster
    k_meansPLOT1 <- ggbiplot(pc,
                             obs.scale = 1,
                             var.scale = 1,
                             groups = kCluster$cluster,
                             ellipse = TRUE,
                             circle = TRUE,
                             ellipse.prob = 0.68)
    k_meansPLOT1 <- k_meansPLOT1 + scale_color_discrete(name = '')
    k_meansPLOT1 <- k_meansPLOT1 + theme(legend.direction = 'horizontal',
                                         legend.position = 'top') +
                                  ggtitle('Variants grouped by k-means clusters')
    output$plot1 <- renderPlot({
      k_meansPLOT1
    })
    
    # kmeans clustering plot ## color points by class
    k_meansPLOT2 <- ggbiplot(pc,
                             obs.scale = 1,
                             var.scale = 1,
                             groups = as.factor(df1$binary_clinvar_class),
                             ellipse = TRUE,
                             circle = TRUE,
                             ellipse.prob = 0.68)
    k_meansPLOT2 <- k_meansPLOT2 + scale_color_discrete(name = '')
    k_meansPLOT2 <- k_meansPLOT2 + theme(legend.direction = 'horizontal',
                                         legend.position = 'top') +
                          ggtitle('Variants grouped by clinvar class')
    #print(k_meansPLOT2)
    output$plot2 <- renderPlot({
      k_meansPLOT2
    })
    
    ###################### 
    ###################### 
    ###################### GFMM # not working right now, troubleshoot eventually
    
    # maybe with Shiny add way for user to select default or EDDA
    
    # on scaled data ## default
    #if (input$radio == 1) {
    #mod <- MclustDA(data_scaled, df1$binary_clinvar_class)
    #summary(mod)}
    
    # on scaled data ## EDDA
    #if (input$radio == 2) {
    #mod <- MclustDA(data_scaled, df1$binary_clinvar_class, modelType = "EDDA")
    #summary(mod)}
    
    ##cross validation for GFMM
    
    #cv <- cvMclustDA(mod, nfold = 10)
    #unlist(cv[3:6])
    # ce      se.ce      brier   se.brier 
    # 0.04347826 0.01653451 0.03763904 0.01277613
    
    ######################
    ###################### 
    ###################### naive bayes
    
    # make dataframe with class
    str(data_scaled)
    data_scaled_df2 <- as.data.frame(data_scaled)
    data_scaled_df2$class <- as.factor(df1$binary_clinvar_class)
    str(data_scaled_df2)
    
    # train test split
    set.seed(input$whatSeed)
    ind <- sample(2, nrow(data_scaled_df2), replace = T, prob = c(input$num3, 1-input$num3))
    train <- data_scaled_df2[ind == 1,]
    test <- data_scaled_df2[ind == 2,]
    str(train)
    str(test)
    
    # drop class from train ## crap gotta fix this, can't hard code with Shiny app!!! 
    drops <- c("class")
    train_noClass <- train[ , !(names(train) %in% drops)]
    
    # train model
    #model <- naive_bayes(class ~ ., data = train, usekernel = T) 
    model = train(train_noClass,train$class,'nb',trControl=trainControl(method='cv',number=3))
    model
    plot(model) 
    
    # predict
    p <- predict(model, train, type = 'prob')
    head(cbind(p, train))
    
    # CM
    p1 <- predict(model, train)
    tab1 <- table(p1, train$class)
    tab1
    # p1   B  P
    # B   23  1
    # P    0 82
    
    1 - sum(diag(tab1)) / sum(tab1)
    
    # 0.009433962
    
    p2 <- predict(model, test)
    tab2 <- table(p2, test$class)
    tab2
    # p2   B  P
    # B    9  2
    # P    0 44
    
    1 - sum(diag(tab2)) / sum(tab2)
    
    # 0.03636364
    
    # save train and test dataframes to show shawn and sujatha
    
    #write.csv(train,'/Users/calhoujd/Downloads/AVE_CombData_Aug2024/TP53_Fayer_variant_list/TP53_naiveBayes_train_v1.csv')
    #write.csv(test,'/Users/calhoujd/Downloads/AVE_CombData_Aug2024/TP53_Fayer_variant_list/TP53_naiveBayes_test_v1.csv')
    
    p <- predict(model, data_scaled_df2, type = 'prob')
    str(p)
    p_df <- as.data.frame(p)
    p_df$variant <- df1$hgvs_pro
    p_df$class <- df1$binary_clinvar_class
    str(p_df)
    #write.csv(p_df,'/Users/calhoujd/Downloads/AVE_CombData_Aug2024/TP53_Fayer_variant_list/TP53_naiveBayes_output.csv')
    no_na_df$NBpred <- p
    
    ######################
    ###################### 
    ###################### random forest in R
    
    
    
    str(train)
    
    bestmtry <- tuneRF(train,train$class,ntreeTry=input$num4,stepFactor = input$num5, improve = input$num6, trace=T, plot= T) 
    
    model <- randomForest(class~.,data= train)
    
    model 
    
    importance(model) 
    
    varImpPlot(model)
    
    pred_test <- predict(model, newdata = test, type= "class")
    
    pred_test
    
    str(test)
    confusionMatrix(table(pred_test,test$class))
    
    pred_train_prob <- predict(model, newdata = train, type= "prob")
    
    rf_train_df <- as.data.frame(cbind(train,pred_train_prob))
    
    pred_test_prob <- predict(model, newdata = test, type= "prob")
    
    rf_test_df <- as.data.frame(cbind(test,pred_test_prob))
    
    data_scaled_df <- as.data.frame(data_scaled)
    predict_RFall <- predict(model, newdata = data_scaled_df, type= "prob")
    predict_RFall_df <- as.data.frame(predict_RFall)
    df
    
    rf_cv <- rfcv(train_noClass, train$class, cv.fold=3, step=0.5)
    #print(rf_cv$error.cv)
    
    #write.csv(rf_train_df,'/Users/calhoujd/Downloads/AVE_CombData_Aug2024/TP53_Fayer_variant_list/TP53_RF_train_output.csv')
    #write.csv(rf_test_df,'/Users/calhoujd/Downloads/AVE_CombData_Aug2024/TP53_Fayer_variant_list/TP53_RF_test_output.csv')

    no_na_df$RFpred <- predict_RFall_df$P

    
    #######################
    #######################
    #######################
    
    # write loop to generate ROC curve for each individual MAVE
    ## and also the integrated dataset
    
    # make a new subset df with added PC1, GFMM?, naive bayes, and random forest
    ## not sure how to make probabilites with GFMM, similar problem to kmeans clustering
    ### so leave off here, but can include output or plots using GFMM / kmeans
    df_subset2 <- df_subset
    df_subset2$PC1 <- df_wPC1$PC1
    df_subset2$NB <- p_df$P
    df_subset2$RF <- predict_RFall_df$P
    
    #run on first column
    ROC_firstCol <- roc(as.factor(df1$binary_clinvar_class), df_subset2[[1]])
    my.coords <- coords(ROC_firstCol, "best", ret = "all", transpose = FALSE)
    
    # setup empty dataframe to accept data within loop
    big.coords_df <- data.frame(matrix(ncol=24,nrow=length(colnames(df_subset2))))
    colnames(big.coords_df) <- colnames(my.coords)
    rownames(big.coords_df) <- colnames(df_subset2)
    str(big.coords_df)
    
    # loop through each MAVE and calculate ROC curve
    for (i in colnames(df_subset2)){
      ROC_loop <- roc(as.factor(df1$binary_clinvar_class), df_subset2[[i]])
      my.coords_loop <- coords(ROC_loop, "best", ret = "all", transpose = FALSE)
      big.coords_df[i,] <- my.coords_loop
      print(big.coords_df[i,])
      #counter<-counter+1
      #print(counter)
    }
 
    print(model$results$Kappa) # troubleshooting
    
    #######################
    #######################
    ####################### calculate OddsPath
    
    # OddsPath = [P2 * (1 - P1)] / [(1 - P2) * P1] 
    
    # calculate OddsPath
    
    # pre-loop
    ## calculate prior probability
    # count number of path and number of benign
    path_count <- length(which(df1$binary_clinvar_class == "P"))
    benign_count <- length(which(df1$binary_clinvar_class == "B"))
    prior_prob_path <- path_count / (path_count+benign_count)
    prior_prob_benign <- benign_count / (path_count+benign_count)
    
    # loop
    
    # setup empty dataframe to accept data within loop
    oddsPath_path_vec <- c()
    oddsPath_benign_vec <- c()
    counter=1 # not sure if I need a counter
    
    # need to fix this
    for (i in colnames(df_subset2)){
      
      # calculate posterior probability
      # pull from ROC
      post_prob_path <- big.coords_df$tp[counter] / (big.coords_df$tp[counter] + big.coords_df$fp[counter] + 1)
      post_prob_benign <- big.coords_df$fn[counter] / (big.coords_df$tn[counter]+big.coords_df$fn[counter])
      
      # troubleshooting 
      #print(post_prob_path)
      #print(post_prob_benign)
      oddsPath_path_vec[counter] <- (post_prob_path * (1 - prior_prob_path)) / ((1 - post_prob_path) * prior_prob_path)
      # troubleshooting 
      #print(oddsPath_path_vec[counter])
      oddsPath_benign_vec[counter] <- (post_prob_benign * (1 - prior_prob_path)) / ((1 - post_prob_benign) * prior_prob_path)
      # troubleshooting 
      #print(oddsPath_benign_vec[counter])
      counter=counter+1
    }
 
    
    # add OddsPath to output csv
    big.coords_df$OddsPath_path <- oddsPath_path_vec
    big.coords_df$OddsPath_benign <- oddsPath_benign_vec
    
    # 
    
    OddsPath_PLOT1 <- ggplot(big.coords_df, aes(x=rownames(big.coords_df),y=OddsPath_path, fill=as.factor(rownames(big.coords_df)))) +
      geom_col() + theme_bw() + xlab("") +
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),legend.position="none")
    
    output$plot3 <- renderPlot({
      OddsPath_PLOT1
    })
    
    OddsPath_PLOT2 <- ggplot(big.coords_df, aes(x=rownames(big.coords_df),y=OddsPath_benign, fill=as.factor(rownames(big.coords_df)))) +
      geom_col() + theme_bw() + xlab("") +
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),legend.position="none")
    
    output$plot4 <- renderPlot({
      OddsPath_PLOT2
    })
    
    #output$NB_intro <- renderText({ "Naive Bayes Kappa with 3-fold CV:" })
    #output$NB_cv <- renderText({ model$results$Kappa[1] })
    output$RF_intro <- renderText({ "RF 3 fold CV error:" })
    output$RF_cv <- renderText({ rf_cv$error.cv[1] })
    
    #######################
    #######################
    #######################
    
    output$downloadData <- downloadHandler(
      filename = function() {
        paste("data-Metrics-", Sys.Date(), ".csv", sep="")
      },
      content = function(file) {
        write.csv(big.coords_df, file)})
    
    output$downloadData2 <- downloadHandler(
      filename = function() {
        paste("data-VariantLevel-", Sys.Date(), ".csv", sep="")
      },
      content = function(file) {
        write.csv(no_na_df, file)})
  })

}

# Create Shiny app ----
shinyApp(ui, server)